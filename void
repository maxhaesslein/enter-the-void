#!/usr/bin/python


# Enter the Void
# v1, 2023
# by maxhaesslein
# www.maxhaesslein.de

from queue import Queue
from threading import Thread
import numpy as np
import cv2
import pyaudio
import aubio
import os
import sys
import time
import math

options = {
	'screenWidth': 1280,
	'screenHeight': 720,

	'fps': 30,

    'bufferSize': 512,
    'winSizeMultiple': 2, # 2 or 4

	'verbose': True,

	'windowName': 'Enter the Void',

    'audioDevice': 'auto',
    'audioDisplayScaling': 3000,

    'silenceThreshold': 0.18,

    'imagePath': 'assets/',
    'imageExtension': 'png',

}

debugDisplay = {
    'frameCount': False,
    'fps': True,
    'audioFps': False,
    'bpm': False,
    'interval': False,
    'progress': False,
    'pitch': False,
    'volume': False,
    'peaks': False,
}


# setup window
cv2.namedWindow( options['windowName'], cv2.WINDOW_NORMAL )
cv2.resizeWindow( options['windowName'], options['screenWidth'], options['screenHeight'] )
cv2.setWindowProperty( options['windowName'], cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN )

# display startup image
startupImage = np.zeros((options['screenHeight'], options['screenWidth'], 3), np.uint8)
font = cv2.FONT_HERSHEY_TRIPLEX
text = 'loading ...'
fontSize = 1
fontWidth = 1
fontColor = (255,255,255)
textSize = cv2.getTextSize(text, font, fontSize, fontWidth)[0]
textX = int((options['screenWidth'] - textSize[0]) / 2)
textY = int((options['screenHeight'] - textSize[1]*2))
cv2.putText( img=startupImage, text=text, org=(textX, textY), fontFace=font, fontScale=fontSize, color=fontColor, thickness=fontWidth, lineType=cv2.LINE_AA )

cv2.imshow(options['windowName'], startupImage)
cv2.waitKey(60)

# get audio input options
pa = pyaudio.PyAudio()
if options['verbose']:
    print("listing microphones")
    for i in range(pa.get_device_count()):
        dev = pa.get_device_info_by_index(i)
        print(i,dev['name'],dev['maxInputChannels'])
        if options['audioDevice'] == 'auto' and dev['maxInputChannels'] > 0:
            options['audioDevice'] = i
            print('  automatically set audio device to '+str(i))
if pa.get_device_count() < options['audioDevice']:
    print('audio device index is out of range (found '+str(pa.get_device_count())+' audio devices, requested #'+str(options['audioDevice'])+')')
    sys.exit()
audioDevice = pa.get_device_info_by_index(options['audioDevice'])
if audioDevice['maxInputChannels'] <= 0:
    print('audio device has no input channels! aborting')
    print('(use "arecord -l" to check available devices)')
    sys.exit()
audioOptions = {
    'deviceIndex': audioDevice['index'],
    'sampleRate': int(audioDevice['defaultSampleRate']),
    'inputChannels': 1,
    'hopSize': options['bufferSize'],
    'winSize': options['bufferSize']*options['winSizeMultiple'],
}

# get assets
imageFiles = [f for f in os.listdir(options['imagePath']) if os.path.isfile(os.path.join(options['imagePath'], f)) and f.lower().endswith(options['imageExtension'])]

assets = []

for imageFile in imageFiles:
    path = options['imagePath']+imageFile
    image = cv2.imread( path, cv2.IMREAD_UNCHANGED )

    w = 600
    h = round(image.shape[1]/image.shape[0]*w)

    image = cv2.resize( image, (w,h) )

    assets.append(image)


if options['verbose']:
    print('')
    print('---------')
    print('')
    print('-- options --')
    print(options)
    print('')
    print('-- audio options --')
    print(audioOptions)
    print('')
    print('-- assets --')
    print(str(len(assets))+' assets loaded')
    print('')
    print('---------')
    print('')



class AudioProcessor():

    def __init__(self):

        self.fpsMeasurement = options['fps']
        self.previousTime = self.currentTime = time.time()
        self.fpsSmoothing = 0.9

        self.beatQueue = Queue()
        self.pitchQueue = Queue()

        peakBufferSize = int(audioOptions['sampleRate']/options['bufferSize']) # one second worth of bufferSizes
        self.peakBuffer = np.zeros(peakBufferSize)
        self.peakBufferBeat = np.zeros(peakBufferSize)
        self.peakBufferIndex = 0

        volumeBufferSize = int((audioOptions['sampleRate']/options['bufferSize'])/options['fps']) # one frame worth of bufferSizes
        if volumeBufferSize < 1:
            volumeBufferSize = 1
        self.volumeBuffer = np.zeros(volumeBufferSize)
        self.volumeBufferIndex = 0

        self.tempoDetection = aubio.tempo(method='default', buf_size=audioOptions['winSize'], hop_size=audioOptions['hopSize'], samplerate=audioOptions['sampleRate'])

        self.pitchDetection = aubio.pitch(method='default', buf_size=audioOptions['winSize'], hop_size=audioOptions['hopSize'], samplerate=audioOptions['sampleRate'])
        self.pitchDetection.set_unit('cent')

        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(format=pyaudio.paFloat32,
                    input=True,
                    channels=audioOptions['inputChannels'],
                    input_device_index=audioOptions['deviceIndex'],
                    frames_per_buffer=options['bufferSize'],
                    rate=audioOptions['sampleRate'],
                    stream_callback=self.readAudioFrames)


    def readAudioFrames(self, in_data, frame_count, time_info, status):

        signal = np.frombuffer(in_data, dtype=np.float32)

        volume = np.sum(signal**2)/len(signal)

        beat = self.tempoDetection(signal)
        if beat:
            bpm = self.tempoDetection.get_bpm()
            self.beatQueue.put(bpm)

        pitch = self.pitchDetection(signal)
        if pitch[0] != 0:
            self.pitchQueue.put(pitch[0])

        peak = np.abs(np.max(signal)-np.min(signal))
        self.peakBuffer[self.peakBufferIndex] = peak*options['audioDisplayScaling']
        self.peakBufferBeat[self.peakBufferIndex] = beat

        self.peakBufferIndex += 1
        if self.peakBufferIndex >= len(self.peakBuffer):
            self.peakBufferIndex = 0

        self.volumeBuffer[self.volumeBufferIndex] = volume

        self.volumeBufferIndex += 1
        if self.volumeBufferIndex >= len(self.volumeBuffer):
            self.volumeBufferIndex = 0

        self.previousTime, self.currentTime = self.currentTime, time.time()
        fps = math.ceil(1./(self.currentTime-self.previousTime))
        self.fpsMeasurement = (self.fpsMeasurement*self.fpsSmoothing) + (fps*(1.0-self.fpsSmoothing))

        return (in_data, pyaudio.paContinue)

    def getBeat(self):
        if self.beatQueue.qsize() > 0:
            return self.beatQueue.get()
        return False

    def getPitch(self):
        if self.pitchQueue.qsize() > 0:
            return self.pitchQueue.get()
        return False

    def getPeaks(self):
        return self.peakBuffer, self.peakBufferBeat, self.peakBufferIndex

    def getVolume(self):
        volume = np.sum(self.volumeBuffer)
        return volume

    def getFps(self):
        return self.fpsMeasurement

    def stop(self):
        self.stream.stop_stream()
        self.stream.close()
        self.audio.terminate()


def drawDebugDisplay( frame, beatInformation ):

    global volumePercentMeasurement

    font = cv2.FONT_HERSHEY_TRIPLEX

    # frame count debug text
    if debugDisplay['frameCount']:
        text = 'frame #'+str(beatInformation['frameNumber'])+' / next beat at #'+str(int(beatInformation['frameNumber']+beatInformation['framesUntilNextBeat']))
        fontSize = 1
        fontWidth = 1
        fontColor = (255,255,255)
        textSize = cv2.getTextSize(text, font, fontSize, fontWidth)[0]
        textX = 1
        textY = options['screenHeight']-1
        cv2.putText( img=frame, text=text, org=(textX, textY), fontFace=font, fontScale=fontSize, color=fontColor, thickness=fontWidth )

    # fps debug text
    textY = 0
    if debugDisplay['fps']:
        text = str(math.ceil(fpsDisplay))+' main fps'
        fontSize = 1
        fontWidth = 1
        if fpsDisplay < options['fps']*0.8:
            fontColor = (0,0,255)
        elif fpsDisplay < options['fps']*0.9:
            fontColor = (0,255,255)
        else:
            fontColor = (0,255,0)
        textSize = cv2.getTextSize(text, font, fontSize, fontWidth)[0]
        textX = int((options['screenWidth'] - textSize[0]))
        textY = textSize[1]
        cv2.putText( img=frame, text=text, org=(textX, textY), fontFace=font, fontScale=fontSize, color=fontColor, thickness=fontWidth )

    # audioProcessor fps debug text
    if debugDisplay['audioFps']:
        audioFps = audioProcessor.getFps()
        text = str(math.ceil(audioFps))+' audio fps'
        fontSize = 1
        fontWidth = 1
        fontColor = (255,255,255)
        textSize = cv2.getTextSize(text, font, fontSize, fontWidth)[0]
        textX = int((options['screenWidth'] - textSize[0]))
        textY += textSize[1] + 20
        cv2.putText( img=frame, text=text, org=(textX, textY), fontFace=font, fontScale=fontSize, color=fontColor, thickness=fontWidth )

    # bpm debug text
    textY = int(options['screenHeight']/5)
    if debugDisplay['bpm']:
        text = str(round(beatInformation['bpm']))+' bpm'
        fontSize = 3
        fontWidth = 3
        fontColor = (255,255,255)
        if beatInformation['beat']:
            fontColor = (0,0,255)
        textSize = cv2.getTextSize(text, font, fontSize, fontWidth)[0]
        textX = int((options['screenWidth'] - textSize[0])/2)
        cv2.putText( img=frame, text=text, org=(textX, textY), fontFace=font, fontScale=fontSize, color=fontColor, thickness=fontWidth )

    # fpi debug text
    if debugDisplay['interval']:
        text = str(round(beatInformation['framesPerInterval']))+' frames per interval'
        fontSize = 1
        fontWidth = 1
        fontColor = (255,255,255)
        if beatInformation['beat']:
            fontColor = (0,0,255)
        textSize = cv2.getTextSize(text, font, fontSize, fontWidth)[0]
        textX = int((options['screenWidth'] - textSize[0])/2)
        textY += textSize[1]+40
        cv2.putText( img=frame, text=text, org=(textX, textY), fontFace=font, fontScale=fontSize, color=fontColor, thickness=fontWidth )

    # print progress bar until next beat
    y = int(options['screenHeight'] * 2/3)
    x = 300
    w = int(options['screenWidth'] - x*2)
    h = 30
    if debugDisplay['progress']:
        c = (255,255,255)
        if beatInformation['lateBeat']:
            c = (0,0,255)
        lineWidth = 1
        filledWidth = round((1-(beatInformation['framesUntilNextBeat']/beatInformation['framesPerInterval']))*w)
        cv2.rectangle( frame, (x,y), (x+w, y+h), c, lineWidth )
        cv2.rectangle( frame, (x,y), (x+filledWidth, y+h), c, -1 )
        text = 'next beat'
        fontSize = 1
        fontWidth = 1
        fontColor = (255,255,255)
        textSize = cv2.getTextSize(text, font, fontSize, fontWidth)[0]
        cv2.putText( img=frame, text=text, org=(x-textSize[0]-10, int(y+h/2+textSize[1]/2)), fontFace=font, fontScale=fontSize, color=fontColor, thickness=fontWidth )

    # print pitch
    if debugDisplay['pitch']:
        y += h + 20
        c = (255,255,255)
        lineWidth = 1
        filledWidth = int((beatInformation['pitch'])*w/100)
        cv2.rectangle( frame, (x,y), (x+filledWidth, y+h), c, -1 )
        cv2.rectangle( frame, (x,y), (x+w, y+h), c, lineWidth )
        text = 'pitch'
        fontSize = 1
        fontWidth = 1
        fontColor = (255,255,255)
        textSize = cv2.getTextSize(text, font, fontSize, fontWidth)[0]
        cv2.putText( img=frame, text=text, org=(x-textSize[0]-10, int(y+h/2+textSize[1]/2)), fontFace=font, fontScale=fontSize, color=fontColor, thickness=fontWidth )

    # print Volume
    if debugDisplay['volume']:
        y += h + 20
        c = (255,255,255)
        if beatInformation['silence']:
            c = (0,0,255)
        cB = (0,0,0)
        lineWidth = 1
        midW= int(w/2)
        scale = 4
        if beatInformation['volume'] > 0:
            volumePercent = beatInformation['volume']/beatInformation['averageVolume']
        else:
            volumePercent = 1
        if volumePercent > 2:
            volumePercent = 2
        silenceMarker = round(w/2*options['silenceThreshold'])
        volumePercentMeasurement = (volumePercentMeasurement*globalVolumeDisplaySmoothing) + (volumePercent * (1.0-globalVolumeDisplaySmoothing))
        filledWidth = midW*(volumePercentMeasurement-1)
        cv2.rectangle( frame, (x+midW,y), (x+midW+int(filledWidth), y+h), c, -1 )
        cv2.rectangle( frame, (x,y), (x+w, y+h), c, lineWidth )
        cv2.rectangle( frame, (x+silenceMarker,y), (x+silenceMarker+1, y+h), c, lineWidth )
        text = 'volume'
        fontSize = 1
        fontWidth = 1
        fontColor = (255,255,255)
        textSize = cv2.getTextSize(text, font, fontSize, fontWidth)[0]
        cv2.putText( img=frame, text=text, org=(x-textSize[0]-10, int(y+h/2+textSize[1]/2)), fontFace=font, fontScale=fontSize, color=fontColor, thickness=fontWidth )

    # print peaks
    if debugDisplay['peaks']:
        data, beatData, index = audioProcessor.getPeaks()
        index -= 1
        if index < 0:
            index = options['fps']-1

        w = math.ceil(options['screenWidth']/len(data))
        x = options['screenWidth']-w
        y = int(options['screenHeight']*3/5)
        h = 2
        color = (255,255,255)

        for i in range(len(data)):
            y2 = int(data[index])
            o = beatData[index]

            if o:
                cv2.rectangle( frame, (x,y-200), (x+4, y), (0,0,255), -1 )

            cv2.rectangle( frame, (x,y-y2), (x+w, y-y2+h), color, -1 )

            x += w

            index += 1
            if index >= len(data):
                index = 0
            if x >= options['screenWidth']:
                x = 0


    return frame


def getBeatInformation():

    global framesUntilNextBeat, pitchValue, volumeBufferIndex, bpm, framesPerInterval, lateBeat, frameNumber

    framesUntilNextBeat -= 1
    if framesUntilNextBeat < 0:
        lateBeat = True
        framesUntilNextBeat = 0

    beat = audioProcessor.getBeat()
    if beat:
        bpm = beat
        beat = True
        framesPerInterval = framesUntilNextBeat = fps*60/bpm
        lateBeat = False

    if pitchValue > 0:
        oldPitchValue = pitchValue
    pitchValue = 0
    pitch = audioProcessor.getPitch()
    if pitch and pitch > 0:
        pitchValue = pitch

    globalVolume = audioProcessor.getVolume()
    volumeBuffer[volumeBufferIndex] = globalVolume
    volumeBufferIndex += 1
    if volumeBufferIndex >= len(volumeBuffer):
        volumeBufferIndex = 0
    globalAverageVolume = np.sum(volumeBuffer)/len(volumeBuffer)

    silence = False
    if globalVolume < globalAverageVolume * options['silenceThreshold']:
        silence = True

    beatInformation = {
            'fps': fps,
            'frameNumber': frameNumber,
            'bpm': bpm,
            'beat': beat,
            'lateBeat': lateBeat,
            'framesUntilNextBeat': framesUntilNextBeat,
            'framesPerInterval': framesPerInterval,
            'pitch': pitchValue,
            'volume': globalVolume,
            'averageVolume': globalAverageVolume,
            'silence': silence,
    }

    return beatInformation



s_img_index = 0
y_offset = 0
x_offset = 0


def drawFrame( beatInformation ):

    global s_img_index

    frame = np.zeros((options['screenHeight'], options['screenWidth'], 3), np.uint8)

    if False:

        if beatInformation['beat']:
            cv2.rectangle( frame, (0,0), (options['screenWidth'],options['screenHeight']),(0,0,255), -1 )

        x = int(options['screenWidth']/2)
        y = int(options['screenHeight']/2)
        r = int((1-beatInformation['framesUntilNextBeat']/beatInformation['framesPerInterval'])*(options['screenHeight']*0.4))

        cv2.circle( frame,(x,y), r, (255,255,255), -1 )


    if beatInformation['beat'] and not beatInformation['silence']:
        s_img_index += 1
        if s_img_index >= len(assets):
            s_img_index = 0

    s_img = assets[s_img_index]


    if False:

        y1, y2 = y_offset, y_offset + s_img.shape[0]
        x1, x2 = x_offset, x_offset + s_img.shape[1]

        alpha_s = s_img[:, :, 3] / 255.0
        alpha_l = 1.0 - alpha_s

        for c in range(0, 3):
            frame[y1:y2, x1:x2, c] = (alpha_s * s_img[:, :, c] +
                                      alpha_l * frame[y1:y2, x1:x2, c])

    if True:

        # separate the alpha channel from the color channels
        alpha_channel = s_img[:, :, 3] / 255 # convert from 0-255 to 0.0-1.0
        overlay_colors = s_img[:, :, :1]

        # To take advantage of the speed of numpy and apply transformations to the entire image with a single operation
        # the arrays need to be the same shape. However, the shapes currently looks like this:
        #    - overlay_colors shape:(width, height, 3)  3 color values for each pixel, (red, green, blue)
        #    - alpha_channel  shape:(width, height, 1)  1 single alpha value for each pixel
        # We will construct an alpha_mask that has the same shape as the overlay_colors by duplicate the alpha channel
        # for each color so there is a 1:1 alpha channel for each color channel
        #alpha_mask = np.dstack((alpha_channel, alpha_channel, alpha_channel))
        alpha_mask = alpha_channel[:,:,np.newaxis]

        # The background image is larger than the overlay so we'll take a subsection of the background that matches the
        # dimensions of the overlay.
        # NOTE: For simplicity, the overlay is applied to the top-left corner of the background(0,0). An x and y offset
        # could be used to place the overlay at any position on the background.
        h, w = s_img.shape[:2]
        background_subsection = frame[0:h, 0:w]

        # combine the background with the overlay image weighted by alpha
        composite = background_subsection * (1 - alpha_mask) + overlay_colors * alpha_mask

        # overwrite the section of the background image that has been updated
        frame[0:h, 0:w] = composite


    return frame



targetTime = 1./options['fps']
frameNumber = 0
bpm = -1
framesPerInterval = -1
framesUntilNextBeat = -1
lateBeat = False
pitchValue = 0
oldPitchValue = 0

volumeBuffer = np.zeros(options['fps']*10) # 10 seconds worth of frames
volumeBufferIndex = 0
globalVolume = 0
globalAverageVolume = 0
globalVolumeDisplaySmoothing = 0.7
volumePercentMeasurement = 1

fps = options['fps']
fpsDisplay = options['fps']
previousTime = time.time()
currentTime = time.time()


verbose = False

audioProcessor = AudioProcessor()

# main loop
while True:

    startTime = time.time()


    beatInformation = getBeatInformation()

    frame = drawFrame( beatInformation )
    frame = drawDebugDisplay( frame, beatInformation )

    cv2.imshow(options['windowName'], frame)


    frameNumber += 1 # TODO: check, when this rolls over

    previousTime, currentTime = currentTime, time.time()
    fps = round(1./(currentTime-previousTime))
    fpsSmoothing = 0.6
    fpsDisplay = (fpsDisplay * fpsSmoothing) + (fps * (1.0-fpsSmoothing))

    usedTime = time.time() - startTime
    timeLeft = targetTime-usedTime
    sleepTime = int(timeLeft*1000)
    if sleepTime < 1:
        sleepTime = 1

    key = cv2.waitKey(sleepTime)

    if key == 27: # escape
        print('exiting')
        break

    elif key&0xFF == ord('q'):
        debugDisplay['frameCount'] = not debugDisplay['frameCount']
    elif key&0xFF == ord('w'):
        debugDisplay['fps'] = not debugDisplay['fps']
    elif key&0xFF == ord('e'):
        debugDisplay['audioFps'] = not debugDisplay['audioFps']
    elif key&0xFF == ord('r'):
        debugDisplay['bpm'] = not debugDisplay['bpm']
    elif key&0xFF == ord('t'):
        debugDisplay['interval'] = not debugDisplay['interval']
    elif key&0xFF == ord('z'):
        debugDisplay['progress'] = not debugDisplay['progress']
    elif key&0xFF == ord('u'):
        debugDisplay['pitch'] = not debugDisplay['pitch']
    elif key&0xFF == ord('i'):
        debugDisplay['volume'] = not debugDisplay['volume']
    elif key&0xFF == ord('o'):
        debugDisplay['peaks'] = not debugDisplay['peaks']

    elif key&0xFF == ord('v'):
        verbose = not verbose


print('cleaning up ..')

audioProcessor.stop()
cv2.destroyAllWindows()

print('bye!')

